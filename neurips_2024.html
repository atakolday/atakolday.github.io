<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Modeling Dynamic Social Vision Highlights Gaps Between Deep Learning and Humans</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/responsive_style.css">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
</nav>
        

<section class="hero">
  <div class="hero-body">
    <div class="container is-widescreen">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Modeling Dynamic Social Vision Highlights Gaps Between Deep Learning and Humans</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <strong>Kathy Garcia</strong><sup>*</sup>,</span>&nbsp;
            <span class="author-block">
                <a href="https://emaliemcmahon.github.io">Emalie McMahon</a><sup>*</sup>,&nbsp;
            </span>
            <span class="author-block">
              <a href="https://colinconwell.github.io">Colin Conwell</a>,</span>&nbsp;
            <span class="author-block">
              <a href="https://bonnerlab.org">Michael F. Bonner</a>,&nbsp;
            </span>
            <span class="author-block">
              <a href="https://www.isiklab.org">Leyla Isik</a>&nbsp;
            </span> 
          </div>
          <br>
          <div class="is-size-3 contributions">
            <span class="author-block">*authors contributed equally</span>
          </div>
          <div class="is-size-4 publication-authors">
            <span class="author-block">Johns Hopkins University</span>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Deep learning models trained on computer vision tasks are now widely considered the most successful models of human vision to date. 
            The majority of work that supports this idea evaluates how accurately these models predict brain and behavioral responses to static images of objects and natural scenes. 
            Beyond the laboratory, however, the visual world is highly dynamic, and far less work has focused on evaluating the accuracy of deep learning models in predicting responses 
            to stimuli that move, and that involve more complicated, higher-order phenomena like social interactions. Here, we present a dataset of natural videos and captions involving 
            complex multi-agent interactions, and we benchmark 350+ image, video, and lan- guage models on behavioral and neural responses to the videos. As with prior work, we find that 
            many vision models reach the noise ceiling in predicting visual scene features and responses along the ventral visual stream (often considered the primary neural substrate of 
            object and scene recognition). In contrast, image models poorly predict human action and social interaction ratings and neural responses in the lateral stream (a neural pathway 
            increasingly theorized as specializing in dynamic, social vision). Language models (given human sentence captions of the videos) predict action and social ratings better than 
            either image or video models, and performance seems to be driven by both noun and verb information in the captions. On the other hand, language models are relatively poor models 
            of neural responses, while video models show surprising gains in neural predictivity. However, even the top performing models in lateral brain regions explain less variance than 
            in ventral regions of the brain, particularly in the superior temporal sulcus, a known hub for social visual processing. Together these results identify a major gap in AI's ability 
            to match the human brain and behavior and highlight the importance of studying vision in dynamic, natural contexts.<br><br><br><br>
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->


    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Methodological Overview</h2>
        <div class="content has-text-justified">
          <div style="text-align: center;">
            <img src="static/figures/neurips/neurips_fig1.jpeg" alt="Methods Overview">

            <p class="caption" style="width: 100%; text-align: center;"><b>Figure 1:</b> A summary of our overall approach. We extract representations from over 350 image, video, 
                and language models based on 3 s videos of human social actions or their captions. We then use model representations to predict human behavioral ratings and the neural 
                responses recorded using fMRI to the videos.<br><br><br></p>
          </div>
        </div>
        <br><br>
      </div>
    </div>  

    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Similarity of DNN with Social Features in Visual Scenes</h2>
        <div class="content has-text-justified">
          <div style="text-align: center;">
            <br>
            <img src="static/figures/fig3.png" style="display: block; margin-left: auto; margin-right: auto;">
            <br>
            <p class="caption"><b>Figure 3: Correlation Between AlexNet Layers and Behavioral Ratings</b> - Correlations based on cRSA between each convolutional layer of AlexNet and two behavioral ratings: <b>Expanse (left)</b> and <b>Communication (right)</b>. 
              Bar height represents the Spearman correlation coefficient for each layer, numbered 1 through 5 along the horizontal axis, indicating the sequence of the convolutional layers from input to output within the network. Blue bars indicate 
              statistically significant correlations (<em>p</em> &lt; 0.05 based on permutation test). Gray bars signify statistically insignificant results. </p>
          </div>
        </div>
        <br><br>
      </div>
    </div>  

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">DNN Architecture Comparison Across Ventral & Lateral Stream ROIs</h2>
        <div class="content has-text-justified">
          <div style="text-align: center;">
            <br>
            <img src="static/figures/fig4a.png" style="display: block; margin-left: auto; margin-right: auto;">
            <img src="static/figures/fig4b.png" style="display: block; margin-left: auto; margin-right: auto;">
            <img src="static/figures/fig4c.png" style="display: block; margin-left: auto; margin-right: auto;">
            <img src="static/figures/fig4d.png" style="display: block; margin-left: auto; margin-right: auto;">
            <img src="static/figures/fig4e.png" style="display: block; margin-left: auto; margin-right: auto;">
            <img src="static/figures/fig4f.png" style="display: block; margin-left: auto; margin-right: auto;">
            <img src="static/figures/fig4g.png" style="display: block; margin-left: auto; margin-right: auto;">
            <img src="static/figures/fig4h.png" style="display: block; margin-left: auto; margin-right: auto;">
            <br>
            <p class="caption"><b>Figure 4: veRSA ROI correlation with different DNN architectures</b> - veRSA results for each ROI, including: (a) early visual cortex (EVC), (b) middle temporal area (MT), (c) extrastriate body area (EBA), 
              (d) lateral occipital complex (LOC), (e) posterior superior temporal sulcus (PSTS), (f) anterior superior temporal sulcus (ASTS), (g) parahippocampal place area (PPA), and (h) fusiform face area (FFA). The data points represent 
              individual DNN models. The color coding of these points denotes the architecture type of each DNN model. The shape of the data points indicates the type of training each model has undergone. The gray bars across the graphs establish 
              the lower and upper bounds of split-half reliability. </p>
          </div>
        </div>
        <br><br>
      </div>
    </div>  

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Overall DNN-Brain Correspondence in the Lateral and Ventral ROIs</h2>
        <div class="content has-text-justified">
          <div style="text-align: center;">
            <br>
            <img src="static/figures/fig5.png" style="display: block; margin-left: auto; margin-right: auto;">

            <p class="caption"><b>Figure 5: cRSA Correlation DNNs vs. ROIs</b> - The figure presents the overall correlation between DNNs and for lateral and ventral ROIs using classic RSA. The Spearman r correlation values, indicating the strength of the 
              relationship between DNN model features and neural activity, are plotted for different brain regions within both the lateral and ventral streams. Each data point represents the cRSA Spearman r correlation for a specific DNN model within a particular 
              Region of Interest (ROI). The colors of the dots correspond to the architectural type of the DNNs: blue for convolutional, orange for transformer, green for hybrid, and red for mixer. The black line across the plot indicates the average correlation 
              across all DNNs for each ROI, providing a visual summary of the overall trend. From left to right, the ROIs within the lateral stream include the Early Visual Cortex (EVC), Middle Temporal area (MT), Lateral Occipital complex (LOC), 
              Extrastriate Body Area (EBA), Posterior Superior Temporal Sulcus (PSTS), and Anterior Superior Temporal Sulcus (ASTS). For the ventral stream, the Parahippocampal Place Area (PPA) and Fusiform Face Area (FFA) are shown. </p>
            
            <br>
            <img src="static/figures/fig6.png" style="display: block; margin-left: auto; margin-right: auto;">

            <p class="caption"><b>Figure 6: veRSA Across Diverse DNN Architectures</b> - This figure illustrates the results from a voxel encoding Representational Similarity Analysis (veRSA) applied to the same DNNs in Figure 4. 
              Plotting conventions follow those in Figure 5. </p>
          </div>
        </div>
        <br><br>
      </div>
    </div>  

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Hierarchical Correspondence in the Lateral Stream</h2>
        <div class="content has-text-justified">
          <div style="text-align: center;">
            <br>
            <img src="static/figures/fig7a.png" style="display: block; margin-left: auto; margin-right: auto;">
            <img src="static/figures/fig7a.png" style="display: block; margin-left: auto; margin-right: auto;">

            <p class="caption"><b>Figure 7: DNN Layers and Brain ROIs</b> - These panels illustrate the relationship between the depth of the best fitting layer within various DNNs and specific brain ROIs in the lateral and ventral visual streams. 
              The layer depth ranges from 0, representing the first layer of the DNN, to 1, denoting the last layer. The individual data points, color-coded by Architecture Type, indicate the normalized depth of the layer that best matches the activation 
              pattern for each ROI. The black line represents the mean layer depth across all models for each ROI, providing a visual summary of the average hierarchical correspondence. The division of ROIs into Lateral and Ventral Streams facilitates a 
              comparison of hierarchical processing patterns across these distinct pathways. </p>
          </div>
        </div>
        <br><br>
      </div>
    </div>  

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">What's next?.. Image Size Tranformations on Hierarchical Representations!</h2>
        <div class="content has-text-justified">
          <div style="text-align: center;">
            <br>
            <img src="static/figures/fig8.png" style="display: block; margin-left: auto; margin-right: auto;">

            <p class="caption"><b>Figure 8: Effect of transforming input images during model feature extraction</b> - This figure illustrates the results from a voxel encoding Representational Similarity Analysis (veRSA) applied to both CLIP RN50 and AlexNet. 
              The two lines represent the results when transformations to the input images were applied vs no transformations (Standacrd). Follows the same plotting conventions as figure 6 and 7b, respectively. </p>
          </div>
        </div>
        <br><br>
      </div>
    </div>   -->

  </div>
</section>



</body>
</html>
