<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Kathy Garcia</title>

    <meta name="author" content="Ata Kolday">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Kathy Garcia
                </p>
                <br>
                <p>I'm a 2nd year Ph.D. student studying Computational Cognitive Science at <a href="https://cogsci.jhu.edu/people/graduate-students/">Johns Hopkins University</a>.
                </p>
                <p>
                  Throughout my Ph.D., I have been a graduate researcher in the <a href="https://www.isiklab.org">Computational Cognitive Neuroscience Lab</a>, and adivsed by <a href="https://cogsci.jhu.edu/directory/leyla-isik/">Leyla Isik </a>.
                </p>
                <p style="text-align:center">
                  <a href="mailto:kgarci1188@gmail.com">Email</a> &nbsp;/&nbsp;
                  <a href="data/KATHY_CV_APR24.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/kathy-garcia-01/">LinkedIn</a> &nbsp;/&nbsp;
                  <a href="https://twitter.com/NeuroKathyG">Twitter</a> &nbsp;/&nbsp;
                  <a href="https://github.com/garciakathy/">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/Garcia-Kathy-187x271.png"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/Garcia-Kathy-187x271.png" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  I'm interested in human vision, deep neural networks (DNNs), and dynamic social perception. My research aims to find biologically plausible computational models for dynamic and social visual perception. 
                  Therefore, most of my work thus far has been on large-scale benchmarking of DNNs for dynamic social perception, focusing on the recently proposed "lateral" visual stream.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src="images/neurips_header.jpg">
      </td>
      <td width="75%" valign="center">
        <p>
          <span class="papertitle">Modeling Dynamic Social Vision Highlights Gaps Between Deep Learning and Humans
          </span>
          <br>
          <strong>Kathy Garcia</strong>,
          <a href="https://emaliemcmahon.github.io">Emalie McMahon</a>,
          <a href="https://colinconwell.github.io">Colin Conwell</a>,
          <a href="https://bonnerlab.org">Michael F. Bonner</a>,
          <a href="https://www.isiklab.org">Leyla Isik</a>
          <br>
          <em> Preprint</em>
          <br>
        </p>
        <div class="paper" id="">
          <a href="https://osf.io/preprints/psyarxiv/4mpd9"Paper</a> &nbsp/&nbsp
          <!-- Paper &nbsp/&nbsp -->
          <a href="https://atakolday.github.io/modeling_dynamic.html">Project Page</a> &nbsp/&nbsp
          <a href="https://github.com/Isik-lab/SIfMRI_modeling">code & data</a>
        </div>

        <p>
        We present a dataset of natural videos and captions involving complex multi-agent interactions, and we benchmark 350+ image, video, 
        and language models on behavioral and neural responses to the videos. Together these results identify a major gap in AI's ability 
        to match the human brain and behavior and highlight the importance of studying vision in dynamic, natural contexts.
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src="images/paper1fig1.jpg">
      </td>
      <td width="75%" valign="center">
        <a href="https://atakolday.github.io/lsbm.html">
            <span class="papertitle">Large-scale Deep Neural Network Benchmarking in Dynamic Social Vision
        </span>
        </a>
        <br>
        <strong>Kathy Garcia</strong>,
        <a href="https://colinconwell.github.io">Colin Conwell</a>,
        <a href="https://emaliemcmahon.github.io">Emalie McMahon</a>,
        <a href="https://bonnerlab.org">Michael F. Bonner</a>,
        <a href="https://www.isiklab.org">Leyla Isik</a>
        <br>
        <em> <a href="https://www.visionsciences.org/presentation/?id=716">VSS, 2024 (Talk presentation)</a></em>
        <br>
        <p></p>
        <p>
        Large-scale benchmarking of 300+ DNNs with diverse architectures, objectives, and training sets, against fMRI responses to a curated dataset of 200 naturalistic social videos, with a focus on the "lateral" visual stream.
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src="images/proj2_fig2.jpg">
      </td>
      <td width="75%" valign="center">
        <a href="https://atakolday.github.io/fmri_svr.html">
            <span class="papertitle">Predicting Dimensional Symptoms of Psychopathology from Task-Based fMRI using Support Vector Regression
        </span>
        </a>
        <br>
        <strong>Kathy Garcia</strong>,
        Zach Anderson,&nbsp;
        Iris Ka-Yi Chat,&nbsp;
        Katherine S.F. Damme,&nbsp;
        Katherine Young,&nbsp;<br>
        Susan Y. Bookheimer,&nbsp;
        Richard Zinbarg,&nbsp;
        Michelle Craske,&nbsp;
        Robin Nusslock&nbsp;
        <br>
        <em>SfN Global Connectome, 2021 (Virtual poster presentation)</em>
        <br>
        <p></p>
        <p>
          This study develops a novel machine learning approach using Support Vector Regression (SVR) to explore potential biomarkers in fMRI data for symptoms of anxiety and depression, 
          finding that MID task-fMRI data does not accurately predict these symptoms, with results indicating a poor model fit.
        </p>
      </td>
    </tr>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <h2>Miscellaneous</h2>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/JHU.jpg">
              </td>
              <td width="85%" valign="center">
                Teaching Assistant, Cognitive Neuropsychology of Visual Perception, Spring 2024
                <br>
                Teaching Assistant, Neuroimaging Methods in High-Level Vision, Fall 2023
                <br>
                Teaching Assistant, Computational Cognitive Neuroscience of Vision, Spring 2023
              </td>
            </tr>

          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>