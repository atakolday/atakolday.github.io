<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="">
  <meta name="keywords" content="Large Vision Models, LVM, LVMs, Large Vision Model">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Large Vision Models</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href="./static/images/favicon.svg"> -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
</nav>
        

<section class="hero">
  <div class="hero-body">
    <div class="container is-widescreen">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Large-scale Deep Neural Network Benchmarking for Dynamic Social Vision</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <strong>Kathy Garcia</strong><sup>*</sup>,</span>&nbsp;
            <span class="author-block">
              <a href="https://colinconwell.github.io">Colin Conwell</a>,</span>&nbsp;
            <span class="author-block">
              <a href="https://emaliemcmahon.github.io">Emalie McMahon</a>,&nbsp;
            </span>
            <span class="author-block">
              <a href="https://bonnerlab.org">Michael F. Bonner</a>,&nbsp;
            </span><br>
            <span class="author-block">
              <a href="https://www.isiklab.org">Leyla Isik</a>&nbsp;
            </span> 
          </div>

          <div class="is-size-4 publication-authors">
            <span class="author-block">Department of Cognitive Science, Johns Hopkins University</span>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>





<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Many Deep Neural Networks (DNNs) with diverse architectures and learning objectives have yielded high brain similarity and hierarchical correspondence to ventral stream responses to static images. 
            However, they have not been evaluated on dynamic social scenes, which are thought to be processed primarily in the recently proposed lateral visual stream. Here, we ask whether DNNs are similarly good 
            models of processing in the lateral stream and superior temporal sulcus as they are of the ventral stream. To investigate this, we employ large-scale deep neural network benchmarking against fMRI responses 
            to a curated dataset of 200 naturalistic social videos. We examine over 300 DNNs with diverse architectures, objectives, and training sets. Notably, we find a hierarchical correspondence between DNNs and lateral 
            stream responses: earlier DNN layers correlate better with earlier visual areas (including early visual cortex and middle temporal cortex), middle layers match best with mid-level regions (extrastriate body area 
            and lateral occipital cortex), and finally later layers in the most anterior regions (along the superior temporal sulcus). Pairwise permutation tests further confirm significant differences in average depth of the best 
            layer match between each region of interest. Interestingly, we find no systematic differences between diverse network types in terms of either hierarchical correspondence or absolute correlation with neural data, 
            suggesting drastically different network factors (like learning objective and training dataset) play little role in a network's representational match to the lateral stream. Finally, while the best DNNs provided a 
            representational match to ventral stream responses near the level of the noise ceiling, DNN correlations were significantly lower in all lateral stream regions. Together, these results provide evidence for a 
            feed-forward visual hierarchy in the lateral stream and underscore the need for further refinement in computational models to adeptly capture the nuances of dynamic, social visual processing.<br><br><br><br>
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3" style="white-space: nowrap;">Social Interaction Perception in the Brain</h2>
        <div class="content has-text-justified">
          <img src="static/figures/fig1.png" alt="Fig1" style="display: block; margin-left: auto; margin-right: auto;" width="400" height="400">
          <p class="caption"><b>Figure 1:</b> Lateral Stream and the Ventral Stream.</p>
        </div>
        <br><br>
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Methodological Overview</h2>
        <div class="content has-text-justified">
          <div style="text-align: center;">
            <img src="static/figures/fig2.png" alt="Methods Overview">

            <p class="caption" style="width: 100%; text-align: center;"><b>Figure 2: From left to right -</b> Participants watch 3 sec naturalistic, social videos and collection of behavioral ratings of videos on predefined social interaction dimensions. 
              Diverse range of DNN models and extraction of their feature activation in response to video stimuli (bottom). Next, these features are then structured into representational dissimilarity matrices (RDMs) by computing pairwise Pearson 
              correlation distances across all pairs of stimuli (middle). ROI identification within the brain's lateral and ventral stream followed after standard preprocessing pipeline using GLM (top). The resultant RDMS for each model layer are 
              compared against the neural RDMS derived from fMRI data (top), indicating the similarity between the model's representation of the stimuli and brain's response patterns. The veRSA approach (bottom, end) builds on the cRSA framework 
              by incorporating a voxel-wise predictive modeling step (middle, bottom) by fitting the model features to the brain data using ridge regression. Predicted brain responses are then converted to RDMs, using Pearson correlation distances. 
              These predicted RDMs are compared to the neural RDM to compute a weighted correlation score that reflects the model's predictive accuracy at the voxel level.</p>
          </div>


          <style>
  
          
          .image-container {
            display: flex;
            flex-wrap: wrap; 
            justify-content: space-between; 
          }

          .image {
            flex: 1; 
            margin: 10px;
            max-width: calc(25% - 20px); 
          }

          .image img {
            width: 150%; 
            height: auto;
          }

          .caption {
        width: 80%; /* Adjust the width as needed */
        margin: 10px auto; /* Centers the caption and adds space above */
        text-align: center;
    }
          </style>
      

        </div>

        <!-- <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3"></h2>
            <div class="content has-text-justified"> -->
              <div class="image-container">
                <div class="image">
                  <img src="static/images/semantic_segmentation_plot.jpg" alt="Image 1">
                </div>
                <div class="image">
                  <img src="static/images/depth_estimation_plot.jpg" alt="Image 2">
                </div>
                <div class="image">
                  <img src="static/images/Surface_Normal_plot.jpg" alt="Image 3">
                </div>
                <div class="image">
                  <img src="static/images/Edge_Detection_plot.jpg" alt="Image 4">

                </div>

              </div>
              <p class="caption" style="width: 100%; text-align: center;"><b>Figure 3. Larger LVMs perform better on downstream tasks.</b> We evaluate LVMs of varying sizes on 4 different downstream tasks, following the 5 shot setting on the ImageNet validation set and report the perplexity. We find that perplexity decreases with larger models across all tasks, indicating the strong scalability.</p>
              <br><br>
            <!-- </div>

          </div> -->
        <!-- </div> -->

        <div style="text-align: center;">
          <img src="static/images/dataset_ablation.jpg" alt="Scalability Part 1" >
          <p class="caption" style="width: 100%; text-align: center;">Figure 4. We evaluate the perplexity of 4 models trained on different sub-components of our datasets on tasks using the ImageNet validation set. All models are 3B parameters and all evaluations are conducted in the 5-shot setting. We can see that the model benefits from each single images, videos and annotations, demonstrating the importance of our training dataset diversity.</p>

          <!-- <p class="caption"><b>Figure 2. Training loss for the 300M, 600M, 1B, and 3B models.</b> All models are trained on 420B tokens, which correspond to 1.64B images. The training scales well with model sizes.</p> -->
          <!-- <p class="total-caption" style="width: 100%; text-align: center;"><b>Total Caption:</b> ThisThisThisThisThisThisThisThisThisThis is the total caption for all four images.</p> -->

        </div>
      </div>
    </div>
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3" style="white-space: nowrap;">Results, everything in prompts.</h2>
        <div class="content has-text-justified">
          <img src="static/images/videos.jpg" alt="Visual Sentences">
          <p class="caption" style="width: 100%; text-align: center;"><b>Frame predictions.</b> LVM predicts the next frame (marked in red) given previous video frames as prompt. The results reveal the LVM can predict the video frames while considering dynamic objects and camera motion.</p>

          <img src="static/images/complex_task_2.jpg" alt="Visual Sentences"> 
          <p class="caption" style="width: 100%; text-align: center;"><b>In and out of distribution prompting examples.</b> Every row is a prompt that contains a sequence of images interleaved with annotations, followed by a query. The last image is predicted by the model (marked in red). The last 5 rows show examples where the query image is out of distribution (painting, sketch, etc) for the task it was trained for. </p>

          <img src="static/images/complex_task_3.jpg" alt="Visual Sentences"> 
          <p class="caption" style="width: 100%; text-align: center;"><b>Compositing & novel tasks.</b> compositing
            several tasks together within a single prompt. Here, we
            demonstrate the rotation task together with the novel key-
            point correspondence task and request the model to continue
            the pattern. </p>



        </div>
        <br><br>
        <h2 class="title is-3" style="white-space: nowrap;">Miscellaneous Prompts. Guess what's next?</h2>

            <div class="content has-text-justified">

              
             <div style="text-align: center;"> 
              <img src="static/images/Guess_2.jpg" alt="Visual Sentences" width="500" height="500"> 
              <p class="caption" style="width: 100%; text-align: center;"><b>Tasks that are not always easily describable in language</b> </p>
              <br>
            
            </div>

    
              <img src="static/images/raven_2.jpg" alt="Visual Sentences"> 
              <p class="caption" style="width: 100%; text-align: center;"><b>Non-verbal IQ tests.</b></p>
              <br>
              <div style="text-align: center;">
                <img src="static/images/misc.jpg" alt="Visual Sentences" width="500" height="500">
                <p class="caption" style="width: 100%; text-align: center;">A variety of simple vision
                  tasks, such as object replication (top), relighting (middle), and
                  zooming in (bottom), can be simply specified via a suitably chosen
                  visual sentence prompt that expresses the task to the LVM</p>
            </div>

      </div>


    </div>

  </div>
</section>



</body>
</html>
